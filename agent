#!/home/faisal/EventMarketDB/venv/bin/python
"""Universal Agent CLI with MCP support for all providers

Usage:
  ./agent -p "prompt" --provider {claude|openai|gemini} [--model MODEL] [--skills SKILL]

Examples:
  ./agent -p "What is 2+2?" --provider openai
  ./agent -p "Count Report nodes in Neo4j" --provider gemini --model gemini-2.5-flash
  ./agent -p "Analyze why PLCE moved +84% after 8-K 0001104659-24-098778" --provider openai --model gpt-4o --skills earnings-attribution
  ./agent -p "Analyze AAPL earnings" --provider gemini --model gemini-2.5-flash --skills earnings-attribution

Sub-agent combinations (all work):
  Primary: OpenAI/Gemini can spawn any sub-agent (Claude/OpenAI/Gemini) with skills
  OpenAI -> OpenAI, OpenAI -> Gemini, OpenAI -> Claude
  Gemini -> OpenAI, Gemini -> Gemini, Gemini -> Claude

Combination that doesn't work:
  Claude (primary) -> OpenAI/Gemini (sub-agent)
  Reason: ./agent --provider claude calls claude -p (Claude Code CLI), which has its
  own Task tool - not our subagent tool. So Claude can't spawn OpenAI/Gemini.
  Fix (if needed): Replace Claude CLI with Anthropic SDK so we can inject subagent tool.
"""
import argparse, subprocess, json, os, re
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
SKILLS_DIR = PROJECT_DIR / ".claude" / "skills"
MCP_URL = "http://localhost:31380/mcp"

def get_secret(key):
    if v := os.environ.get(key): return v
    try:
        r = subprocess.run(["kubectl", "get", "secret", "eventtrader-secrets", "-n", "processing",
            "-o", f"jsonpath={{.data.{key}}}"], capture_output=True, text=True)
        if r.stdout:
            import base64; return base64.b64decode(r.stdout).decode()
    except: pass
    return None

def load_skills(names):
    if not names: return None
    parts = []
    for n in names.split(","):
        for p in [f"{n.strip()}/SKILL.md", f"{n.strip()}.md"]:
            if (SKILLS_DIR / p).exists(): parts.append((SKILLS_DIR / p).read_text()); break
    return "\n\n---\n\n".join(parts) if parts else None

# MCP HTTP Client
class MCPClient:
    def __init__(self, url=MCP_URL):
        self.url, self.session_id, self.tools_cache = url, None, None

    def _call(self, method, params=None, id=1):
        import requests
        headers = {"Content-Type": "application/json", "Accept": "application/json, text/event-stream"}
        if self.session_id: headers["mcp-session-id"] = self.session_id
        body = {"jsonrpc": "2.0", "method": method, "id": id}
        if params: body["params"] = params
        try:
            r = requests.post(self.url, json=body, headers=headers, timeout=30)
            if "mcp-session-id" in r.headers: self.session_id = r.headers["mcp-session-id"]
            for line in r.text.split("\n"):
                if line.startswith("data: "): return json.loads(line[6:]).get("result")
        except: pass
        return None

    def init(self):
        return self._call("initialize", {"protocolVersion": "2024-11-05", "capabilities": {},
            "clientInfo": {"name": "agent", "version": "1.0"}})

    def list_tools(self):
        if not self.tools_cache:
            if not self.session_id: self.init()
            result = self._call("tools/list", id=2)
            self.tools_cache = result.get("tools", []) if result else []
        return self.tools_cache

    def call_tool(self, name, args):
        if not self.session_id: self.init()
        result = self._call("tools/call", {"name": name, "arguments": args}, id=3)
        if result and "content" in result:
            return "\n".join(c.get("text", "") for c in result["content"] if c.get("type") == "text")
        return json.dumps(result) if result else "[mcp error]"

mcp = MCPClient()

# Tools
def subagent(provider, prompt, skills="", model="", reasoning=""):
    """Spawn sub-agent, optionally capturing reasoning to file"""
    import uuid
    cmd = [str(PROJECT_DIR / "agent"), "-p", prompt, "--provider", provider]
    if skills: cmd += ["--skills", skills]
    if model: cmd += ["--model", model]
    if reasoning:
        cmd += ["--reasoning", reasoning]
        # Auto-save reasoning to transcript file
        transcript_dir = PROJECT_DIR / "earnings-analysis" / "transcripts"
        transcript_dir.mkdir(parents=True, exist_ok=True)
        reasoning_file = transcript_dir / f"subagent-{uuid.uuid4().hex[:8]}.txt"
        cmd += ["--reasoning-file", str(reasoning_file)]
    result = subprocess.run(cmd, capture_output=True, text=True, cwd=PROJECT_DIR)
    return result.stdout or "[no output]"

def parallel_subagents(agents):
    """
    Spawn multiple sub-agents in PARALLEL.

    Args:
        agents: JSON string of list of agent specs, each with:
            - provider: "claude" | "openai" | "gemini"
            - prompt: The prompt for the agent
            - skills: Optional skills to load
            - model: Optional model override

    Returns:
        JSON string with results from all agents

    Example:
        parallel_subagents('[
            {"provider": "openai", "prompt": "Query 1"},
            {"provider": "openai", "prompt": "Query 2"},
            {"provider": "gemini", "prompt": "Query 3"}
        ]')
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import uuid

    # Parse agents list
    if isinstance(agents, str):
        agent_list = json.loads(agents)
    else:
        agent_list = agents

    def run_one(spec):
        cmd = [str(PROJECT_DIR / "agent"), "-p", spec["prompt"], "--provider", spec.get("provider", "openai")]
        if spec.get("skills"): cmd += ["--skills", spec["skills"]]
        if spec.get("model"): cmd += ["--model", spec["model"]]
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=PROJECT_DIR)
        return {"prompt": spec["prompt"][:50], "result": result.stdout or "[no output]"}

    # Run all in parallel using ThreadPoolExecutor
    results = []
    with ThreadPoolExecutor(max_workers=len(agent_list)) as executor:
        futures = {executor.submit(run_one, spec): i for i, spec in enumerate(agent_list)}
        for future in as_completed(futures):
            idx = futures[future]
            try:
                results.append({"index": idx, **future.result()})
            except Exception as e:
                results.append({"index": idx, "error": str(e)})

    # Sort by original index
    results.sort(key=lambda x: x["index"])
    return json.dumps(results, indent=2)

def read_file(path):
    """Read file contents"""
    try:
        return Path(path).read_text()
    except Exception as e:
        return f"[error reading {path}: {e}]"

def write_file(path, content):
    """Write content to file"""
    try:
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        Path(path).write_text(content)
        return f"[written to {path}]"
    except Exception as e:
        return f"[error writing {path}: {e}]"

def list_directory(path="."):
    """List directory contents"""
    try:
        items = []
        for p in sorted(Path(path).iterdir()):
            prefix = "[DIR]" if p.is_dir() else "[FILE]"
            items.append(f"{prefix} {p.name}")
        return "\n".join(items) if items else "[empty directory]"
    except Exception as e:
        return f"[error listing {path}: {e}]"

def search_files(pattern, path="."):
    """Search for files matching glob pattern"""
    try:
        matches = list(Path(path).rglob(pattern))
        return "\n".join(str(m) for m in matches[:100]) if matches else "[no matches]"
    except Exception as e:
        return f"[error searching: {e}]"

BUILTIN = {
    "subagent": {"fn": subagent, "desc": "Spawn sub-agent with any provider, optionally capture reasoning",
        "params": {"provider": {"type": "string", "enum": ["claude", "openai", "gemini"]},
                   "prompt": {"type": "string"}, "skills": {"type": "string"}, "model": {"type": "string"},
                   "reasoning": {"type": "string", "enum": ["low", "medium", "high", "x-high"], "description": "Enable reasoning capture"}},
        "required": ["provider", "prompt"]},
    "read_file": {"fn": read_file, "desc": "Read file contents",
        "params": {"path": {"type": "string", "description": "File path to read"}},
        "required": ["path"]},
    "write_file": {"fn": write_file, "desc": "Write content to file",
        "params": {"path": {"type": "string", "description": "File path to write"},
                   "content": {"type": "string", "description": "Content to write"}},
        "required": ["path", "content"]},
    "list_directory": {"fn": list_directory, "desc": "List directory contents",
        "params": {"path": {"type": "string", "description": "Directory path (default: current)"}},
        "required": []},
    "search_files": {"fn": search_files, "desc": "Search for files matching glob pattern",
        "params": {"pattern": {"type": "string", "description": "Glob pattern (e.g. *.txt, **/*.py)"},
                   "path": {"type": "string", "description": "Starting directory (default: current)"}},
        "required": ["pattern"]},
    "parallel_subagents": {"fn": parallel_subagents, "desc": "Spawn multiple sub-agents in PARALLEL (for concurrent queries)",
        "params": {"agents": {"type": "string", "description": "JSON array of agent specs: [{provider, prompt, skills?, model?}, ...]"}},
        "required": ["agents"]}
}

def get_tools():
    tools = dict(BUILTIN)
    for t in mcp.list_tools():
        props = t.get("inputSchema", {}).get("properties", {})
        tools[t["name"]] = {"desc": t.get("description", ""), "params": {k: {"type": v.get("type", "string"),
            "description": v.get("description", "")} for k, v in props.items()},
            "required": t.get("inputSchema", {}).get("required", []), "mcp": True}
    return tools

def run_tool(name, args):
    import sys
    print(f"[DEBUG] Tool called: {name}", file=sys.stderr)
    tools = get_tools()
    if name not in tools: return f"[unknown: {name}]"
    if tools[name].get("mcp"): return mcp.call_tool(name, args)
    return tools[name]["fn"](**{k: v for k, v in args.items() if v})

# Providers
def run_claude(prompt, system=None, model="sonnet", reasoning=None):
    if reasoning:
        # Try Anthropic SDK first for thinking capture
        try:
            import anthropic
            api_key = get_secret("ANTHROPIC_API_KEY")
            if api_key:
                model_map = {
                    "sonnet": "claude-sonnet-4-20250514",
                    "opus": "claude-opus-4-20250514",
                    "opus-4.5": "claude-opus-4-5-20251101",
                    "haiku": "claude-haiku-4-20250514",
                }
                model_id = model_map.get(model, model)
                client = anthropic.Anthropic(api_key=api_key)
                # Max budget: 128k tokens (Claude 3.7 with beta), fits in 200k context (Claude 4)
                budget_map = {"low": 5000, "medium": 10000, "high": 32000, "x-high": 100000}
                budget = budget_map.get(reasoning, 10000)
                # max_tokens must be > budget_tokens
                max_tokens = max(budget + 8000, 16000)

                kwargs = {
                    "model": model_id,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": prompt}],
                    "thinking": {"type": "enabled", "budget_tokens": budget}
                }
                if system:
                    kwargs["system"] = system

                # Use streaming for high budget to avoid timeout
                if budget >= 20000:
                    thinking_text = ""
                    response_text = ""
                    with client.messages.stream(**kwargs) as stream:
                        for event in stream:
                            if hasattr(event, 'type'):
                                if event.type == 'content_block_start':
                                    pass  # Block started
                                elif event.type == 'content_block_delta':
                                    delta = event.delta
                                    if hasattr(delta, 'thinking'):
                                        thinking_text += delta.thinking
                                    elif hasattr(delta, 'text'):
                                        response_text += delta.text
                    if thinking_text:
                        return (response_text, thinking_text.strip())
                    return response_text
                else:
                    response = client.messages.create(**kwargs)
                    thinking_text = ""
                    response_text = ""
                    for block in response.content:
                        if block.type == "thinking":
                            thinking_text += block.thinking + "\n"
                        elif block.type == "text":
                            response_text += block.text

                    if thinking_text:
                        return (response_text, thinking_text.strip())
                    return response_text
        except Exception as e:
            import sys
            print(f"[WARNING] Anthropic SDK failed ({e}), falling back to CLI", file=sys.stderr)

        # Fallback: CLI with JSON output to capture thinking
        prompt = f"ultrathink: {prompt}"
        full = f"System context:\n{system}\n\n---\n\nTask: {prompt}" if system else prompt
        r = subprocess.run(["claude", "-p", full, "--output-format", "json", "--dangerously-skip-permissions"],
                          capture_output=True, text=True, cwd=PROJECT_DIR)
        if r.stdout:
            try:
                data = json.loads(r.stdout)
                # Extract thinking and response from JSON
                thinking_parts = []
                response_parts = []
                messages = data.get("messages", [])
                for msg in messages:
                    if msg.get("role") == "assistant":
                        for block in msg.get("content", []):
                            if block.get("type") == "thinking":
                                thinking_parts.append(block.get("thinking", ""))
                            elif block.get("type") == "text":
                                response_parts.append(block.get("text", ""))
                if thinking_parts:
                    return ("\n".join(response_parts), "\n".join(thinking_parts))
                return "\n".join(response_parts) if response_parts else data.get("result", r.stdout)
            except json.JSONDecodeError:
                return r.stdout
        return r.stderr or "[no output]"

    # Use CLI (default, no reasoning requested)
    full = f"System context:\n{system}\n\n---\n\nTask: {prompt}" if system else prompt
    r = subprocess.run(["claude", "-p", full, "--dangerously-skip-permissions"], capture_output=True, text=True, cwd=PROJECT_DIR)
    return r.stdout or r.stderr

def run_openai(prompt, system=None, model="gpt-5.2", reasoning=None):
    from openai import OpenAI
    client = OpenAI(api_key=get_secret("OPENAI_API_KEY"))

    # GPT-5.x uses Responses API, older models use Chat Completions
    use_responses_api = model.startswith("gpt-5")

    if use_responses_api:
        # Responses API for GPT-5.x
        tools = get_tools()
        tool_defs = [{"type": "function", "name": n, "description": t["desc"],
            "parameters": {"type": "object", "properties": t["params"], "required": t.get("required", [])}
        } for n, t in tools.items()]

        full_prompt = f"{system}\n\n{prompt}" if system else prompt
        kwargs = {"model": model, "input": full_prompt, "tools": tool_defs}
        if reasoning:
            # summary: "auto" requires org verification at platform.openai.com
            kwargs["reasoning"] = {"effort": reasoning, "summary": "auto"}

        summary_enabled = "summary" in kwargs.get("reasoning", {})
        for _ in range(15):
            try:
                r = client.responses.create(**kwargs)
            except Exception as e:
                # If org not verified for summaries, retry without summary
                if "verified" in str(e).lower() and summary_enabled:
                    import sys
                    print("[WARNING] Org not verified for reasoning summaries. "
                          "Verify at: https://platform.openai.com/settings/organization/general",
                          file=sys.stderr)
                    kwargs["reasoning"] = {"effort": reasoning}
                    summary_enabled = False
                    r = client.responses.create(**kwargs)
                else:
                    raise

            # Check for tool calls
            tool_calls = [item for item in r.output if getattr(item, 'type', None) == 'function_call']
            if not tool_calls:
                # Extract reasoning summary from ResponseReasoningItem
                reasoning_parts = []
                for item in r.output:
                    if getattr(item, 'type', None) == 'reasoning':
                        # summary can be a list of parts or a string
                        summary = getattr(item, 'summary', None)
                        if summary:
                            if isinstance(summary, list):
                                for part in summary:
                                    if hasattr(part, 'text'):
                                        reasoning_parts.append(part.text)
                                    elif isinstance(part, str):
                                        reasoning_parts.append(part)
                            else:
                                reasoning_parts.append(str(summary))

                response_text = r.output_text if hasattr(r, 'output_text') else str(r.output)
                if reasoning and reasoning_parts:
                    return (response_text, "\n".join(reasoning_parts))
                return response_text

            # Handle tool calls - collect all results and chain via previous_response_id
            tool_outputs = []
            for tc in tool_calls:
                args = json.loads(tc.arguments) if isinstance(tc.arguments, str) else dict(tc.arguments)
                result = run_tool(tc.name, args)
                tool_outputs.append({
                    "type": "function_call_output",
                    "call_id": tc.call_id,
                    "output": str(result)
                })
            # Chain with previous response
            kwargs["input"] = tool_outputs
            kwargs["previous_response_id"] = r.id
        return "[max turns]"
    else:
        # Chat Completions API for older models (gpt-4o, etc.)
        tools = get_tools()
        otool = [{"type": "function", "function": {"name": n, "description": t["desc"],
            "parameters": {"type": "object", "properties": t["params"], "required": t.get("required", [])}
        }} for n, t in tools.items()]
        msgs = ([{"role": "system", "content": system}] if system else []) + [{"role": "user", "content": prompt}]

        for _ in range(15):
            r = client.chat.completions.create(model=model, messages=msgs, tools=otool)
            m = r.choices[0].message
            msgs.append(m.model_dump())
            if not m.tool_calls:
                return m.content
            for tc in m.tool_calls:
                msgs.append({"role": "tool", "tool_call_id": tc.id,
                            "content": run_tool(tc.function.name, json.loads(tc.function.arguments))})
        return "[max turns]"

def run_gemini(prompt, system=None, model="gemini-3-pro-preview", reasoning=None):
    from google import genai
    from google.genai import types
    # Gemini doesn't expose thinking tokens, so we use structured prompt engineering
    if reasoning:
        reasoning_prefix = """Think step by step. Structure your response as follows:

<reasoning>
[Your detailed step-by-step reasoning process here]
</reasoning>

<answer>
[Your final answer here]
</answer>

Now solve: """
        prompt = reasoning_prefix + prompt
    tools = get_tools()
    decls = [types.FunctionDeclaration(name=n, description=t["desc"],
        parameters=types.Schema(type="OBJECT", properties={k: types.Schema(type=v.get("type", "string").upper(),
            enum=v.get("enum"), description=v.get("description")) for k, v in t["params"].items()},
            required=t.get("required", []))) for n, t in tools.items()]
    client = genai.Client(api_key=get_secret("GEMINI_API_KEY"))
    config = types.GenerateContentConfig(system_instruction=system, tools=[types.Tool(function_declarations=decls)])
    contents = [types.Content(role="user", parts=[types.Part(text=prompt)])]
    for _ in range(15):
        r = client.models.generate_content(model=model, contents=contents, config=config)
        part = r.candidates[0].content.parts[0]
        if not part.function_call:
            text = part.text
            # Extract reasoning and answer from structured response
            if reasoning and "<reasoning>" in text and "</reasoning>" in text:
                import re
                reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', text, re.DOTALL)
                answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
                if reasoning_match and answer_match:
                    return (answer_match.group(1).strip(), reasoning_match.group(1).strip())
                elif reasoning_match:
                    # No answer tags, use everything after reasoning as answer
                    reasoning_text = reasoning_match.group(1).strip()
                    answer_text = re.sub(r'<reasoning>.*?</reasoning>', '', text, flags=re.DOTALL).strip()
                    return (answer_text, reasoning_text)
            return text
        fc = part.function_call
        contents.append(r.candidates[0].content)
        contents.append(types.Content(role="user", parts=[types.Part(function_response=types.FunctionResponse(
            name=fc.name, response={"result": run_tool(fc.name, dict(fc.args))}))]))
    return "[max turns]"

RUNNERS = {"claude": run_claude, "openai": run_openai, "gemini": run_gemini}
DEFAULTS = {"claude": "sonnet", "openai": "gpt-5.2", "gemini": "gemini-3-pro-preview"}

def format_output(response, reasoning=None, reasoning_file=None):
    """Format output with optional reasoning section"""
    if reasoning:
        if reasoning_file:
            Path(reasoning_file).write_text(reasoning)
            print(f"[Reasoning saved to {reasoning_file}]", file=__import__('sys').stderr)
        output = f"=== REASONING ===\n{reasoning}\n\n=== RESPONSE ===\n{response}"
    else:
        output = response
    return output

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("-p", "--prompt", required=True)
    p.add_argument("--provider", choices=["claude", "openai", "gemini"], default="claude")
    p.add_argument("--model")
    p.add_argument("--skills")
    p.add_argument("--system")
    p.add_argument("--reasoning", choices=["low", "medium", "high", "x-high"],
                   help="Enable reasoning capture (OpenAI: native, Gemini: prompt-based)")
    p.add_argument("--reasoning-file", help="Save reasoning to file")
    a = p.parse_args()
    result = RUNNERS[a.provider](a.prompt, a.system or load_skills(a.skills),
                                  a.model or DEFAULTS[a.provider], a.reasoning)
    if isinstance(result, tuple):
        response, reasoning = result
        print(format_output(response, reasoning, a.reasoning_file))
    else:
        print(result)
