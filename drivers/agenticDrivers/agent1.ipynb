{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd4fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you started Jupyter from the EventMarketDB directory with .env loaded\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment\")\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f458d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('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',\n",
       " 'AIzaSyCJ1T2evhtbiGnYLrlh0Q7VWwzPDQVXHYU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "PERPLEXITY_API_KEY, GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bcbd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26456da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Nvidia’s latest 10-Q, the \"Management’s Discussion and Analysis\" (MD&A) highlighted exceptional revenue growth, especially in the Data Center segment, continued high demand for its AI GPUs (notably the Hopper and Blackwell series), supply and production updates, evolving gross margins, and shareholder return plans.\n",
      "\n",
      "Key points from the MD&A include:\n",
      "\n",
      "- **Record Revenue and Segment Performance:** Nvidia reported total Q2 fiscal 2025 revenue of $30.0 billion, substantially driven by its Data Center segment, which alone accounted for $26.3 billion. This growth was attributed to robust demand for AI hardware from cloud service providers and enterprise customers, particularly for the Hopper GPU platform[1][2].\n",
      "\n",
      "- **Margins and Profitability:** While revenue growth was strong, gross margins contracted slightly, with the Data Center non-GAAP gross margin at 78% and total gross margin guidance for the following quarter expected at around 75%. Management cited changes in product mix within the Data Center business as a key factor affecting margins[1].\n",
      "\n",
      "- **Supply, Demand, and Product Transition:** Management discussed ongoing improvement in supply for Hopper GPUs. However, for its next-generation Blackwell GPUs, demand continues to significantly exceed supply, with the production ramp now scheduled for Q4 and fiscal 2026[1]. Nvidia highlighted that Blackwell GPUs provide 3-5 times more AI throughput than Hopper in power-limited Data Centers, underscoring its future growth potential[1].\n",
      "\n",
      "- **Outlook:** Nvidia guided Q3 FY2025 revenue above consensus, projecting $33.1 billion in total revenue with Data Center expected around $29.1 billion, but with a slight decrease in gross margin due to the mix shift in high-volume, lower-margin AI products[1].\n",
      "\n",
      "- **Shareholder Returns:** The company announced a $50 billion share buyback, a move noted in the MD&A, which some analysts interpreted as a signal that management sees limited higher-return investment opportunities in the near term[1].\n",
      "\n",
      "- **Trends and Risks:** Management reiterated strong secular tailwinds for AI and high-performance computing but also acknowledged potential risks including supply constraints for advanced chips, competition, changing regulatory environments, and cyclicality in technology spending[1][2].\n",
      "\n",
      "- **Other Segments:** The Gaming, Professional Visualization, and Automotive segments also contributed, though growth in Gaming was notably less than the Data Center segment[2].\n",
      "\n",
      "These MD&A themes collectively reflect how Nvidia is navigating explosive AI-driven demand, rapid product evolution, and balancing growth with margin headwinds and capital allocation decisions. For detailed language and risk disclosures, the official 10-Q document should be referenced directly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def perplexity_sec_search(query: str) -> str:\n",
    "    \"\"\"Uses Perplexity SEC mode to answer finance questions from filings.\"\"\"\n",
    "    api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"sonar-pro\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "        \"search_mode\": \"sec\",\n",
    "        \"web_search_options\": {\"search_context_size\": \"medium\"},\n",
    "    }\n",
    "    resp = requests.post(\"https://api.perplexity.ai/chat/completions\", headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "res = perplexity_sec_search.invoke(\"What did Nvidia report in 'Management’s Discussion and Analysis' in its latest 10-Q?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699e8726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc507b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152667/3227139208.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25daf4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - IBM',\n",
       "  'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "  'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\nLLM applications: By using LangGraph’s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.',\n",
       "  'score': 0.94608605},\n",
       " {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
       "  'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'What is LangGraph?\\n==================\\n\\nLangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\n\\nKey Concepts\\n============ [...] Conclusion\\n==========\\n\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n\\nLangchain\\n\\nLlm\\n\\nAI\\n\\nChatbots\\n\\nPython\\n\\n\\n--------------\\n\\n\\n\\nKiss Tibor (Cloud Mentor) he/him\\n\\nSep 20, 2024',\n",
       "  'score': 0.94174546},\n",
       " {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] LangGraph can be used to build a wide range of applications.\\n\\n### Chatbots\\n\\nLangGraph is ideal for developing sophisticated chatbots that can handle a wide array of user requests. By leveraging multiple LLM agents, these chatbots can process natural language queries, provide accurate responses, and seamlessly switch between different conversation topics. The ability to manage state and coordinate interactions ensures that the chatbot maintains context and delivers a coherent user experience. [...] LangGraph enables us to create stateful, multi-actor applications utilizing LLMs as easily as possible. It extends the capabilities of LangChain, introducing the ability to create and manage cyclical graphs, which are pivotal for developing sophisticated agent runtimes. The core concepts of LangGraph include: graph structure, state management, and coordination.\\n\\n### Graph structure\",\n",
       "  'score': 0.92715925}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e31cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78d80d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0SkUHh3IjDMahUBriq8mHceKHOjX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6b68e6d-7d0a-4a97-b844-a59084866126-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages          \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a18532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
