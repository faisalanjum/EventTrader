{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 8-K XBRL Extraction Pipeline - Interactive Testing\n\nTest every component of the extraction pipeline step-by-step.\n\n| Section | What it Tests | Dependencies |\n|---------|--------------|--------------|\n| 1-5 | Individual functions (parsing, validation) | None |\n| 6 | Full postprocessor with mock data | None |\n| 7 | XBRL catalog fetch from Neo4j | `neo4j`, `python-dotenv` |\n| 8-9 | **Real LangExtract call** + postprocessing | `langextract`, `neo4j` |\n| 10 | Save results (JSONL + HTML) | `langextract` |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add paths and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure imports resolve\n",
    "BASE = os.path.dirname(os.path.abspath('.'))\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'Experiments'))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Value Parsing\n\nExtracts numeric values from text using priority-based matching:\n- Currency + multiplier: `$2.75 billion` → 2,750,000,000\n- Parentheses negative: `($450 million)` → -450,000,000\n- Rejects percentage-only for non-ratio units"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor import parse_number_from_text\n",
    "\n",
    "# Test cases: (text, unit, expected, description)\n",
    "test_cases = [\n",
    "    (\"net income of $2.75 billion\", None, 2_750_000_000, \"Currency + multiplier\"),\n",
    "    (\"revenue increased 10% to $2.75 billion\", None, 2_750_000_000, \"Priority over percentage\"),\n",
    "    (\"($2.3 billion)\", None, -2_300_000_000, \"Parentheses negative\"),\n",
    "    (\"loss of ($450 million)\", None, -450_000_000, \"Parentheses negative with context\"),\n",
    "    (\"EPS of $4.80\", None, 4.80, \"Small currency amount\"),\n",
    "    (\"$2,750,000,000 in assets\", None, 2_750_000_000, \"Large number with commas\"),\n",
    "    (\"2.75 billion shares\", None, 2_750_000_000, \"Multiplier without currency\"),\n",
    "    (\"margin of 23.5%\", \"pure\", 0.235, \"Percentage with ratio unit\"),\n",
    "    (\"margin of 23.5%\", \"USD\", None, \"Percentage rejected for USD\"),\n",
    "    (\"(10%) decline\", None, None, \"Percentage in parens - not negative\"),\n",
    "]\n",
    "\n",
    "print(\"VALUE PARSING TESTS\\n\" + \"=\"*60)\n",
    "for text, unit, expected, desc in test_cases:\n",
    "    value, error = parse_number_from_text(text, unit)\n",
    "    \n",
    "    if expected is None:\n",
    "        passed = value is None\n",
    "    else:\n",
    "        passed = value is not None and abs(value - expected) < 0.01\n",
    "    \n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"{status} {desc}\")\n",
    "    print(f\"   Input: \\\"{text}\\\" (unit={unit})\")\n",
    "    print(f\"   Expected: {expected}, Got: {value}\")\n",
    "    if error:\n",
    "        print(f\"   Error: {error}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Try your own text\n",
    "test_text = \"Record revenue of $29.8 billion, up 19% year over year\"\n",
    "test_unit = \"USD\"\n",
    "\n",
    "value, error = parse_number_from_text(test_text, test_unit)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Unit: {test_unit}\")\n",
    "print(f\"Parsed value: {value:,.2f}\" if value else f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Period Normalization\n\nNormalizes period formats to canonical form:\n- Arrow variants: `-->`, `->`, ` - `, ` to ` → `→`\n- Strips whitespace: `2024-01-01 → 2024-12-31` → `2024-01-01→2024-12-31`\n- Collapses same-day durations to instant: `2024-06-30→2024-06-30` → `2024-06-30`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor import normalize_period, validate_period\n",
    "\n",
    "# Test cases: (input, expected_normalized, should_be_valid, description)\n",
    "test_cases = [\n",
    "    (\"2024-12-31\", \"2024-12-31\", True, \"Instant - no change\"),\n",
    "    (\"2024-01-01→2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Duration - no change\"),\n",
    "    (\"2024-06-30→2024-06-30\", \"2024-06-30\", True, \"Same start/end → instant\"),\n",
    "    (\"2024-01-01-->2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Arrow variant -->\"),\n",
    "    (\"2024-01-01->2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Arrow variant ->\"),\n",
    "    (\"2024-01-01 - 2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Arrow variant ' - '\"),\n",
    "    (\"2024-01-01 → 2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Spaces around arrow\"),\n",
    "    (\"2024-01-01 to 2024-12-31\", \"2024-01-01→2024-12-31\", True, \"Arrow variant ' to '\"),\n",
    "    (\"invalid\", \"invalid\", False, \"Invalid format\"),\n",
    "]\n",
    "\n",
    "print(\"PERIOD NORMALIZATION TESTS\\n\" + \"=\"*60)\n",
    "for input_period, expected, should_be_valid, desc in test_cases:\n",
    "    normalized, was_normalized = normalize_period(input_period)\n",
    "    is_valid = validate_period(normalized)\n",
    "    \n",
    "    passed = (normalized == expected) and (is_valid == should_be_valid)\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    \n",
    "    print(f\"{status} {desc}\")\n",
    "    print(f\"   Input: \\\"{input_period}\\\"\")\n",
    "    print(f\"   Normalized: \\\"{normalized}\\\" (changed={was_normalized})\")\n",
    "    print(f\"   Valid: {is_valid}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Qname Validation\n\nValidates concept qnames against the catalog. Invalid qnames become `UNMATCHED`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor import validate_qname\n",
    "\n",
    "# Mock catalog qnames\n",
    "valid_qnames = {\n",
    "    \"us-gaap:Revenues\",\n",
    "    \"us-gaap:NetIncomeLoss\",\n",
    "    \"us-gaap:Assets\",\n",
    "    \"us-gaap:EarningsPerShareDiluted\",\n",
    "}\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"us-gaap:Revenues\", \"us-gaap:Revenues\", True, \"Valid qname\"),\n",
    "    (\"us-gaap:NetIncomeLoss\", \"us-gaap:NetIncomeLoss\", True, \"Valid qname\"),\n",
    "    (\"us-gaap:FakeConceptXYZ\", \"UNMATCHED\", False, \"Invalid qname → UNMATCHED\"),\n",
    "    (\"UNMATCHED\", \"UNMATCHED\", True, \"UNMATCHED passthrough\"),\n",
    "    (\"\", \"UNMATCHED\", False, \"Empty string → UNMATCHED\"),\n",
    "]\n",
    "\n",
    "print(\"QNAME VALIDATION TESTS\\n\" + \"=\"*60)\n",
    "for qname, expected_result, expected_valid, desc in test_cases:\n",
    "    result, was_valid = validate_qname(qname, valid_qnames)\n",
    "    \n",
    "    passed = (result == expected_result) and (was_valid == expected_valid)\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    \n",
    "    print(f\"{status} {desc}\")\n",
    "    print(f\"   Input: \\\"{qname}\\\"\")\n",
    "    print(f\"   Result: \\\"{result}\\\" (valid={was_valid})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Unit Validation\n\nValidates units against catalog (case-insensitive). Invalid units → REVIEW status."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor import validate_unit\n",
    "\n",
    "# Mock catalog units\n",
    "valid_units = {\"USD\", \"USD/share\", \"shares\", \"pure\"}\n",
    "\n",
    "test_cases = [\n",
    "    (\"USD\", \"USD\", True, \"Valid unit\"),\n",
    "    (\"USD/share\", \"USD/share\", True, \"Valid per-share unit\"),\n",
    "    (\"usd\", \"USD\", True, \"Case-insensitive match\"),\n",
    "    (\"EUR\", \"EUR\", False, \"Invalid unit\"),\n",
    "]\n",
    "\n",
    "print(\"UNIT VALIDATION TESTS\\n\" + \"=\"*60)\n",
    "for unit, expected_result, expected_valid, desc in test_cases:\n",
    "    result, was_valid = validate_unit(unit, valid_units)\n",
    "    \n",
    "    passed = (was_valid == expected_valid)\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    \n",
    "    print(f\"{status} {desc}\")\n",
    "    print(f\"   Input: \\\"{unit}\\\"\")\n",
    "    print(f\"   Result: \\\"{result}\\\" (valid={was_valid})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Status Determination\n\nDetermines extraction status based on validation results:\n- **COMMITTED**: confidence ≥ 0.90 + valid qname + valid unit + valid period + value parsed\n- **CANDIDATE_ONLY**: Valid but low confidence or UNMATCHED concept  \n- **REVIEW**: Parse failure, invalid period, or invalid unit"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor import determine_status\n",
    "from extraction_schema import ExtractionStatus\n",
    "\n",
    "# Test cases: (concept, confidence, value, period_valid, qname_valid, unit_valid, expected_status)\n",
    "test_cases = [\n",
    "    (\"us-gaap:Revenues\", 0.95, 1000.0, True, True, True, ExtractionStatus.COMMITTED, \"High confidence valid\"),\n",
    "    (\"us-gaap:Revenues\", 0.85, 1000.0, True, True, True, ExtractionStatus.CANDIDATE_ONLY, \"Below threshold\"),\n",
    "    (\"UNMATCHED\", 0.95, 1000.0, True, True, True, ExtractionStatus.CANDIDATE_ONLY, \"UNMATCHED concept\"),\n",
    "    (\"us-gaap:Revenues\", 0.95, None, True, True, True, ExtractionStatus.REVIEW, \"Parse failed\"),\n",
    "    (\"us-gaap:Revenues\", 0.95, 1000.0, False, True, True, ExtractionStatus.REVIEW, \"Invalid period\"),\n",
    "    (\"us-gaap:Revenues\", 0.95, 1000.0, True, True, False, ExtractionStatus.REVIEW, \"Invalid unit\"),\n",
    "    (\"us-gaap:FakeConcept\", 0.95, 1000.0, True, False, True, ExtractionStatus.CANDIDATE_ONLY, \"Invalid qname\"),\n",
    "]\n",
    "\n",
    "print(\"STATUS DETERMINATION TESTS\\n\" + \"=\"*60)\n",
    "for concept, conf, value, period_v, qname_v, unit_v, expected, desc in test_cases:\n",
    "    status, committed = determine_status(concept, conf, value, period_v, qname_v, unit_v)\n",
    "    \n",
    "    passed = status == expected\n",
    "    status_str = \"✓\" if passed else \"✗\"\n",
    "    \n",
    "    print(f\"{status_str} {desc}\")\n",
    "    print(f\"   Got: {status.value}, Expected: {expected.value}\")\n",
    "    print(f\"   Committed: {committed}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Full Postprocessor (Mock Data)\n\nTests the complete postprocessor with mock LangExtract-like input. No external dependencies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction_schema import RawExtraction, ExtractionStatus\n",
    "from postprocessor import postprocess\n",
    "\n",
    "# Mock catalog data\n",
    "valid_qnames = {\n",
    "    \"us-gaap:Revenues\",\n",
    "    \"us-gaap:NetIncomeLoss\",\n",
    "    \"us-gaap:EarningsPerShareDiluted\",\n",
    "    \"us-gaap:Assets\",\n",
    "}\n",
    "valid_units = {\"USD\", \"USD/share\", \"shares\", \"pure\"}\n",
    "\n",
    "# Mock extractions (as if from LangExtract)\n",
    "raw_extractions = [\n",
    "    RawExtraction(\n",
    "        extraction_text=\"net income of $2.75 billion for fiscal year 2024\",\n",
    "        char_start=100, char_end=150,\n",
    "        concept_top1=\"us-gaap:NetIncomeLoss\",\n",
    "        matched_period=\"2024-01-01→2024-12-31\",\n",
    "        matched_unit=\"USD\",\n",
    "        confidence=0.95,\n",
    "        reasoning=\"Explicit net income, exact catalog match\",\n",
    "    ),\n",
    "    RawExtraction(\n",
    "        extraction_text=\"diluted EPS of $4.80\",\n",
    "        char_start=200, char_end=220,\n",
    "        concept_top1=\"us-gaap:EarningsPerShareDiluted\",\n",
    "        matched_period=\"2024-01-01→2024-12-31\",\n",
    "        matched_unit=\"USD/share\",\n",
    "        confidence=0.92,\n",
    "        reasoning=\"Diluted EPS value\",\n",
    "    ),\n",
    "    RawExtraction(\n",
    "        extraction_text=\"revenue grew to $8.5 billion\",\n",
    "        char_start=300, char_end=330,\n",
    "        concept_top1=\"UNMATCHED\",\n",
    "        concept_top2=\"us-gaap:Revenues\",\n",
    "        matched_period=\"2024-01-01→2024-12-31\",\n",
    "        matched_unit=\"USD\",\n",
    "        confidence=0.60,\n",
    "        reasoning=\"Revenue concept varies by filer\",\n",
    "    ),\n",
    "    RawExtraction(\n",
    "        extraction_text=\"reported loss of ($450 million)\",\n",
    "        char_start=400, char_end=435,\n",
    "        concept_top1=\"us-gaap:NetIncomeLoss\",\n",
    "        matched_period=\"2024-04-01→2024-06-30\",\n",
    "        matched_unit=\"USD\",\n",
    "        confidence=0.88,\n",
    "        reasoning=\"Quarterly net loss in parentheses\",\n",
    "    ),\n",
    "    RawExtraction(\n",
    "        extraction_text=\"some text without clear number\",\n",
    "        char_start=500, char_end=530,\n",
    "        concept_top1=\"us-gaap:Assets\",\n",
    "        matched_period=\"2024-12-31\",\n",
    "        matched_unit=\"USD\",\n",
    "        confidence=0.70,\n",
    "        reasoning=\"No numeric value found\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run postprocessor\n",
    "processed = postprocess(raw_extractions, valid_qnames, valid_units)\n",
    "\n",
    "print(f\"POSTPROCESSOR TEST\\n\" + \"=\"*60)\n",
    "print(f\"Processed {len(processed)} facts:\\n\")\n",
    "\n",
    "for i, fact in enumerate(processed, 1):\n",
    "    print(f\"--- Fact {i}: {fact.status.value} ---\")\n",
    "    print(f\"  Text: \\\"{fact.extraction_text[:50]}...\\\"\")\n",
    "    print(f\"  Concept: {fact.concept_top1}\")\n",
    "    if fact.concept_top2:\n",
    "        print(f\"  Concept2: {fact.concept_top2}\")\n",
    "    print(f\"  Value: {fact.value_parsed:,.2f}\" if fact.value_parsed else f\"  Value: None\")\n",
    "    print(f\"  Period: {fact.matched_period}\")\n",
    "    print(f\"  Unit: {fact.matched_unit} (valid={fact.unit_valid})\")\n",
    "    print(f\"  Committed: {fact.committed}\")\n",
    "    if fact.parse_error:\n",
    "        print(f\"  Parse Error: {fact.parse_error}\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "committed = sum(1 for f in processed if f.status == ExtractionStatus.COMMITTED)\n",
    "candidate = sum(1 for f in processed if f.status == ExtractionStatus.CANDIDATE_ONLY)\n",
    "review = sum(1 for f in processed if f.status == ExtractionStatus.REVIEW)\n",
    "print(f\"Summary: {committed} COMMITTED, {candidate} CANDIDATE_ONLY, {review} REVIEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Catalog Fetch (Neo4j)\n\nFetches XBRL catalog from Neo4j (READ ONLY - no writes). Contains concepts, units, dimensions, and historical facts for context."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch catalog for a company\n",
    "TICKER = \"DELL\"  # Change this to test different companies\n",
    "\n",
    "try:\n",
    "    from xbrl_catalog import xbrl_catalog, print_catalog_summary\n",
    "    \n",
    "    print(f\"Fetching XBRL catalog for {TICKER}...\")\n",
    "    catalog = xbrl_catalog(TICKER, limit_filings=2)\n",
    "    \n",
    "    print_catalog_summary(catalog)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Install: pip install python-dotenv neo4j\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect catalog data\n",
    "if 'catalog' in dir():\n",
    "    print(f\"Valid Qnames ({len(catalog.concepts)} total, first 20):\")\n",
    "    for qname in list(catalog.concepts.keys())[:20]:\n",
    "        print(f\"  {qname}\")\n",
    "    \n",
    "    print(f\"\\nValid Units ({len(catalog.units)} total):\")\n",
    "    for unit in catalog.units.keys():\n",
    "        print(f\"  {unit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview LLM context\n",
    "if 'catalog' in dir():\n",
    "    context = catalog.to_llm_context()\n",
    "    print(f\"LLM Context ({len(context):,} chars):\\n\")\n",
    "    print(context[:3000])\n",
    "    print(\"\\n... [truncated] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Real LangExtract Call\n\nCalls LangExtract directly to see the **raw extraction output** before any postprocessing. This is the actual LLM call with your prompt and examples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sample 8-K and catalog\nTICKER = \"DELL\"\nSAMPLE_FILE = \"/home/faisal/EventMarketDB/drivers/8K_XBRL_Linking/sample_data/DELL_1571996_2025-08-28_000157199625000096/exhibit_EX-99.1.txt\"\n\n# Load 8-K text\nwith open(SAMPLE_FILE, 'r') as f:\n    sample_8k_text = f.read()\n\nprint(f\"Loaded 8-K: {len(sample_8k_text):,} characters\")\n\n# Fetch catalog\nfrom xbrl_catalog import xbrl_catalog\ncatalog = xbrl_catalog(TICKER, limit_filings=2)\nprint(f\"Catalog: {len(catalog.concepts)} concepts, {len(catalog.units)} units\")\n\n# Build full context (document + catalog)\nllm_context = catalog.to_llm_context()\nfull_context = f\"{sample_8k_text}\\n\\n{llm_context}\"\nprint(f\"Full context: {len(full_context):,} characters\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize LangExtract\nimport langextract as lx\nfrom extraction_config import PROMPT_DESCRIPTION, EXAMPLES\n\nprint(\"LangExtract imported\")\nprint(f\"Model: gemini-2.0-flash\")\nprint(f\"Prompt length: {len(PROMPT_DESCRIPTION)} chars\")\nprint(f\"Examples: {len(EXAMPLES)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run extraction - THIS IS THE REAL LANGEXTRACT CALL\nprint(\"Running LangExtract extraction...\")\nprint(\"(This may take 30-60 seconds depending on document size)\\n\")\n\nannotated_doc = lx.extract(\n    text_or_documents=full_context,\n    prompt_description=PROMPT_DESCRIPTION,\n    examples=EXAMPLES,\n    model_id=\"gemini-2.0-flash\"\n)\n\n# Get extractions from annotated document\nraw_extractions = annotated_doc.extractions\n\nprint(f\"✓ LangExtract returned {len(raw_extractions)} extractions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect RAW LangExtract output (before postprocessing)\nprint(f\"RAW LANGEXTRACT OUTPUT ({len(raw_extractions)} extractions)\\n\" + \"=\"*70)\n\nfor i, ext in enumerate(raw_extractions, 1):\n    print(f\"\\n[{i}] Raw Extraction\")\n    print(f\"    class: {ext.extraction_class}\")\n    print(f\"    text: \\\"{ext.extraction_text[:80]}...\\\"\" if len(ext.extraction_text) > 80 else f\"    text: \\\"{ext.extraction_text}\\\"\")\n    \n    # Handle char_interval (can be None)\n    if ext.char_interval:\n        print(f\"    span: {ext.char_interval.start_pos} - {ext.char_interval.end_pos}\")\n    else:\n        print(f\"    span: N/A\")\n    \n    if ext.alignment_status:\n        print(f\"    alignment: {ext.alignment_status.value}\")\n    \n    attrs = ext.attributes or {}\n    if attrs:\n        print(f\"    --- Attributes ---\")\n        for k, v in attrs.items():\n            if v is not None:\n                print(f\"    {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 9. Postprocess Real Extractions\n\nRuns the postprocessor on actual LangExtract output:\n1. Filters extractions pointing into catalog context (precision fix)\n2. Validates qnames, units, periods\n3. Parses numeric values deterministically\n4. Assigns status: COMMITTED / CANDIDATE_ONLY / REVIEW"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Map raw extractions to RawExtraction dataclass, then postprocess\nfrom extraction_schema import RawExtraction, ExtractionStatus, UNMATCHED\nfrom postprocessor import postprocess\n\n# Get valid qnames and units from catalog\nvalid_qnames = set(catalog.concepts.keys())\nvalid_units = {\"USD\", \"USD/share\", \"shares\", \"pure\"}  # Standard units\n\n# Filter extractions that point into catalog context (precision fix)\nsource_text_length = len(sample_8k_text)\n\n# Map to RawExtraction\nmapped_extractions = []\nfor ext in raw_extractions:\n    # Only process financial_fact extractions\n    if ext.extraction_class != 'financial_fact':\n        continue\n    \n    # Get char positions from char_interval\n    char_start = ext.char_interval.start_pos if ext.char_interval else 0\n    char_end = ext.char_interval.end_pos if ext.char_interval else 0\n    \n    # Skip extractions pointing into catalog context (beyond source text)\n    if char_end > source_text_length:\n        print(f\"Dropping extraction at char_end={char_end} (beyond source text at {source_text_length})\")\n        continue\n    \n    attrs = ext.attributes or {}\n    \n    mapped_extractions.append(RawExtraction(\n        extraction_text=ext.extraction_text,\n        char_start=char_start,\n        char_end=char_end,\n        concept_top1=attrs.get('concept_top1') or UNMATCHED,\n        matched_period=attrs.get('matched_period') or '',\n        matched_unit=attrs.get('matched_unit') or '',\n        confidence=float(attrs.get('confidence', 0.0)),\n        reasoning=attrs.get('reasoning') or '',\n        concept_top2=attrs.get('concept_top2'),\n        matched_dimension=attrs.get('matched_dimension'),\n        matched_member=attrs.get('matched_member')\n    ))\n\nprint(f\"Mapped {len(mapped_extractions)} financial_fact extractions for postprocessing\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run postprocessor\nfrom postprocessor import validate_period\n\nprocessed_facts = postprocess(mapped_extractions, valid_qnames, valid_units)\n\n# Display results\nprint(f\"POSTPROCESSED FACTS ({len(processed_facts)} total)\\n\" + \"=\"*70)\n\nfor i, fact in enumerate(processed_facts, 1):\n    status_icon = \"✓\" if fact.status == ExtractionStatus.COMMITTED else \"○\" if fact.status == ExtractionStatus.CANDIDATE_ONLY else \"✗\"\n    print(f\"\\n[{i}] {status_icon} {fact.status.value}\")\n    print(f\"    Text: \\\"{fact.extraction_text[:60]}...\\\"\")\n    print(f\"    Concept: {fact.concept_top1}\")\n    if fact.concept_top2:\n        print(f\"    Concept2: {fact.concept_top2}\")\n    print(f\"    Value: {fact.value_parsed:,.2f}\" if fact.value_parsed else f\"    Value: None\")\n    print(f\"    Period: {fact.matched_period}\")\n    print(f\"    Unit: {fact.matched_unit} (valid={fact.unit_valid})\")\n    print(f\"    Confidence: {fact.confidence:.2f}\")\n    if fact.parse_error:\n        print(f\"    Parse Error: {fact.parse_error}\")\n\n# Summary\ncommitted = sum(1 for f in processed_facts if f.status == ExtractionStatus.COMMITTED)\ncandidate = sum(1 for f in processed_facts if f.status == ExtractionStatus.CANDIDATE_ONLY)\nreview = sum(1 for f in processed_facts if f.status == ExtractionStatus.REVIEW)\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"SUMMARY: {committed} COMMITTED | {candidate} CANDIDATE_ONLY | {review} REVIEW\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 10. Save Extraction Results\n\nSaves extractions in two formats:\n- **JSONL**: Raw machine-readable format with all extraction data\n- **HTML**: Interactive visualization with entity highlighting in source context",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Output directory\nimport os\nfrom datetime import datetime\n\nOUTPUT_DIR = \"/home/faisal/EventMarketDB/drivers/8K_XBRL_Linking/output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Generate timestamp for filenames\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nbase_name = f\"DELL_8K_{timestamp}\"\n\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Base filename: {base_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save annotated document to JSONL using LangExtract's built-in function\njsonl_path = os.path.join(OUTPUT_DIR, f\"{base_name}.jsonl\")\n\n# save_annotated_documents expects an iterator of AnnotatedDocument\nlx.io.save_annotated_documents(\n    annotated_documents=[annotated_doc],  # List of AnnotatedDocument\n    output_dir=OUTPUT_DIR,\n    output_name=f\"{base_name}.jsonl\"\n)\n\nprint(f\"✓ Saved annotated document to: {jsonl_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate interactive HTML visualization\nhtml_path = os.path.join(OUTPUT_DIR, f\"{base_name}.html\")\n\n# Use LangExtract's visualize - can take AnnotatedDocument directly\nhtml_content = lx.visualize(annotated_doc)\n\n# Save HTML file\nwith open(html_path, \"w\") as f:\n    if hasattr(html_content, 'data'):\n        f.write(html_content.data)\n    else:\n        f.write(str(html_content))\n\nprint(f\"✓ Saved HTML visualization to: {html_path}\")\nprint(f\"\\nOpen in browser to view extractions highlighted in source text.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}